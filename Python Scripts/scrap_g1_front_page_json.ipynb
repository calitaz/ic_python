{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    " Script em Python para captura de comentários no site de notícia g1.globo.com\n",
    " Captura comentários, data do comentário, titulo da noticia, link da noticia e data da noticia\n",
    " Gera dois JSONs, um com os comentarios,data de comentario e link da noticia\n",
    " e outro com titulo, link da noticia e data da noticia\n",
    " Os JSONs são \"linkados\" pelo link da noticia\n",
    " \n",
    " Desenvolvido por: Tauã Gomes de Almeida\n",
    " \n",
    "'''\n",
    "\n",
    "\n",
    "#Bibliotecas\n",
    "import pandas\n",
    "import requests\n",
    "import time\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "#Variáveis\n",
    "records = [] \n",
    "links_reportagem = []\n",
    "titulos = []\n",
    "data_hora = []\n",
    "links_noticia = []\n",
    "list_data_comentarios = []\n",
    "tags_assunto = []\n",
    "links_noticia_comentario = []\n",
    "\n",
    "#Cria uma sessão, evita erros de HTTP request que aconteciam no site ao ter lentidão na requisição\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "#Link para o webdriver chrome\n",
    "driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe')\n",
    "\n",
    "#Funcao para limpar arrays duplicados (não esta sendo usado)\n",
    "def limpaArrays(array):\n",
    "    igual = set()\n",
    "    result = []\n",
    "    for item in array:\n",
    "        if item not in igual:\n",
    "            igual.add(item)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "#Funcao para gerar os arquivos em JSON\n",
    "def gera_json():\n",
    "    \n",
    "    dados_noticia = {\n",
    "        'link_noticia': links_noticia,\n",
    "        'titulo': titulos,\n",
    "        'data_hora': data_hora,\n",
    "        'tags': tags_assunto\n",
    "    }\n",
    "    \n",
    "    \n",
    "    dados_comentarios = {\n",
    "        'link': links_noticia_comentario,\n",
    "        'comentarios': records,\n",
    "        'data_comentario': list_data_comentarios\n",
    "    }\n",
    "    \n",
    "    df = DataFrame(dados_comentarios, columns=['link','comentarios','data_comentario'])\n",
    "    with open('comentarios_.json', 'w', encoding='utf-8') as file:\n",
    "        df.to_json(file, force_ascii=False, orient='index')\n",
    "    \n",
    "\n",
    "    df2 = DataFrame(dados_noticia, columns=['link_noticia','titulo','data_hora','tags'])\n",
    "    with open('dados_noticias_.json', 'w', encoding='utf-8') as file:\n",
    "        df2.to_json(file, force_ascii=False, orient='index')\n",
    "\n",
    "#Funcao de captura das informaçoes         \n",
    "def pega_comentarios():\n",
    "    res = driver.execute_script('return document.documentElement.outerHTML')\n",
    "    soup = BeautifulSoup(res, 'html.parser') \n",
    "    link_noiticia = soup.find(\"link\",{\"itemprop\":\"mainEntityOfPage\"})\n",
    "    href_link_noticia = link_noiticia.get('href')\n",
    "    busca_titulo_noticia = soup.find(\"h1\",{\"class\":\"content-head__title\"})\n",
    "    titulo_noticia = busca_titulo_noticia.text\n",
    "    busca_data_hora = soup.find(\"time\",{\"itemprop\": \"datePublished\"})\n",
    "    data_hora_text = busca_data_hora.text\n",
    "    lista_recente = soup.find(\"div\",\"glbComentarios-lista glbComentarios-lista-recentes\")\n",
    "    if(lista_recente):\n",
    "        lista = lista_recente.find(\"ul\", class_ = \"glbComentarios-lista-todos\")\n",
    "        if(lista):\n",
    "            li = lista.find_all(\"li\", {\"itemtype\":\"http://schema.org/UserComments\"})\n",
    "            for coments_ in li:\n",
    "                coments = coments_.find(\"p\", class_ = \"glbComentarios-texto-comentario\")\n",
    "                if(coments):\n",
    "                    comentarios = coments.contents[0]\n",
    "                    comentarios.strip()\n",
    "                    records.append((comentarios)) \n",
    "                    links_noticia_comentario.append((href_link_noticia))\n",
    "                    data_coments = coments_.find(\"abbr\", {\"itemprop\":\"commentTime\"})\n",
    "                    data_comentarios = data_coments.get('title')\n",
    "                    list_data_comentarios.append((data_comentarios))\n",
    "                   \n",
    "            entities = soup.find(\"a\",{\"class\":\"entities__list-itemLink\"})\n",
    "            if(entities):\n",
    "                tags_assunto.append((entities.text))\n",
    "            else:\n",
    "                entities = soup.find(\"a\",{\"class\":\"header-editoria--link\"})\n",
    "                tags_assunto.append((entities.text))\n",
    "            data_hora.append((data_hora_text))\n",
    "            titulos.append((titulo_noticia))\n",
    "            links_noticia.append((href_link_noticia))\n",
    "    \n",
    "#Funcao para apertar o botao de respostas a comentarios                               \n",
    "def botao_respostas():\n",
    "    while True:\n",
    "        try:\n",
    "            mais_comentarios = driver.find_element_by_xpath(\"//*[@class='glbComentarios-lista glbComentarios-lista-recentes']/ul/li/div[1]/div/div[3]/button[not(contains(@style,'display: none'))]\")\n",
    "            time.sleep(3)\n",
    "            if(mais_comentarios.is_displayed()):\n",
    "                ActionChains(driver).move_to_element(mais_comentarios).click(mais_comentarios).perform()\n",
    "        except NoSuchElementException as e:\n",
    "            break\n",
    "    ##pega_comentarios()   \n",
    "\n",
    "#Funcao para apertar o botao de carregar mais comentarios\n",
    "def botao_carrega():\n",
    "    while True:\n",
    "        try:\n",
    "            carrega_mais = driver.find_element_by_xpath('//*[@id=\"boxComentarios\"]/div[4]/button')\n",
    "            time.sleep(2)\n",
    "            if(carrega_mais.is_displayed()):\n",
    "                ActionChains(driver).move_to_element(carrega_mais).click(carrega_mais).perform()\n",
    "            else:\n",
    "                break\n",
    "        except NoSuchElementException as e:\n",
    "            break\n",
    "    ##botao_respostas()\n",
    "    pega_comentarios() \n",
    "     \n",
    "#Funcao para buscar as reportagens da pagina inicial do g1.globo.com\n",
    "def busca_reportagens(url):\n",
    "    pagina_globo = session.get(url)\n",
    "    soup = BeautifulSoup(pagina_globo.text, 'html.parser')\n",
    "    div_reportagens = soup.find_all(\"div\",{\"class\":\"_et\"})\n",
    "    for inside_divs in div_reportagens:\n",
    "        tag_a = inside_divs.find(\"a\")\n",
    "        href_a = tag_a.get('href')\n",
    "        if(href_a):\n",
    "            valid_href = href_a\n",
    "        links_reportagem.append(tag_a.get('href'))\n",
    "        \n",
    "    for links in limpaArrays(links_reportagem):\n",
    "        try:\n",
    "            pagina = session.get(links)\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "        finally:\n",
    "            soup = BeautifulSoup(pagina.text, 'html.parser')\n",
    "            existe_boxcomentarios = soup.find(\"div\",{\"id\":\"boxComentarios\"})\n",
    "            if(existe_boxcomentarios):\n",
    "                driver.get(links)\n",
    "                driver.maximize_window()\n",
    "                try:\n",
    "                    box_comentarios = driver.find_element_by_id('boxComentarios')\n",
    "                    driver.execute_script('arguments[0].scrollIntoView(true);', box_comentarios)\n",
    "                finally:\n",
    "                    try:\n",
    "                        element = WebDriverWait(driver, 10).until(\n",
    "                            EC.presence_of_element_located((By.CLASS_NAME, \"glbComentarios\"))\n",
    "                        )\n",
    "                    except TimeoutException as timeout:\n",
    "                        print(timeout)\n",
    "                        break\n",
    "                    finally:\n",
    "                        botao_carrega()\n",
    "    driver.quit()\n",
    "\n",
    "#Funcao de inicialização\n",
    "def __init__(url):\n",
    "    busca_reportagens(url)\n",
    "\n",
    "#Start\n",
    "__init__('https://g1.globo.com') \n",
    "gera_json()   \n",
    "        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
