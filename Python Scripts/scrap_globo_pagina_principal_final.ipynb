{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://g1.globo.com/politica/noticia/2019/09/24/congresso-rejeita-parte-dos-vetos-de-bolsonaro-ao-projeto-do-abuso-de-autoridade.ghtml\n",
      "https://g1.globo.com/mundo/noticia/2019/09/24/presidente-da-camara-dos-eua-anuncia-abertura-de-processo-de-impeachment-contra-trump.ghtml\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import time\n",
    "from pandas import DataFrame\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "records = []\n",
    "links_reportagem = []\n",
    "titulos = []\n",
    "data_hora = []\n",
    "links_noticia = []\n",
    "\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount('http://', adapter)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome('C:/chromedriver/chromedriver.exe')\n",
    "\n",
    "def limpaArrays(array):\n",
    "    igual = set()\n",
    "    result = []\n",
    "    for item in array:\n",
    "        if item not in igual:\n",
    "            igual.add(item)\n",
    "            result.append(item)\n",
    "    return result\n",
    "\n",
    "def gera_json():\n",
    "    \n",
    "    Data = {\n",
    "        'link': links_noticia,\n",
    "        'titulo': titulos,\n",
    "        'data_hora': data_hora,\n",
    "        'comentarios': records\n",
    "    }\n",
    "    \n",
    "    df = DataFrame(Data, columns=['link','titulo','data_hora','comentarios'])\n",
    "    with open('pacoteteste2.json', 'w', encoding='utf-8') as file:\n",
    "        df.to_json(file, force_ascii=False, orient='index')\n",
    "\n",
    "def pega_comentarios():\n",
    "    res = driver.execute_script('return document.documentElement.outerHTML')\n",
    "    soup = BeautifulSoup(res, 'html.parser') \n",
    "    link_noiticia = soup.find(\"link\",{\"itemprop\":\"mainEntityOfPage\"})\n",
    "    href_link_noticia = link_noiticia.get('href')\n",
    "    busca_titulo_noticia = soup.find(\"h1\",{\"class\":\"content-head__title\"})\n",
    "    titulo_noticia = busca_titulo_noticia.text\n",
    "    busca_data_hora = soup.find(\"time\",{\"itemprop\": \"datePublished\"})\n",
    "    data_hora_text = busca_data_hora.text\n",
    "    lista_recente = soup.find(\"div\",\"glbComentarios-lista glbComentarios-lista-recentes\")\n",
    "    lista = lista_recente.find(\"ul\", class_ = \"glbComentarios-lista-todos\")\n",
    "    if(lista):\n",
    "        li = lista.find_all(\"li\", {\"itemtype\":\"http://schema.org/UserComments\"})\n",
    "        for coments_ in li:\n",
    "            coments = coments_.find(\"p\", class_ = \"glbComentarios-texto-comentario\")\n",
    "            if(coments):\n",
    "                comentarios = coments.contents[0]\n",
    "                comentarios.strip()\n",
    "                records.append((comentarios)) \n",
    "                data_hora.append((data_hora_text))\n",
    "                titulos.append((titulo_noticia))\n",
    "                links_noticia.append((href_link_noticia))\n",
    "    gera_json()\n",
    "                               \n",
    "def botao_respostas():\n",
    "    while True:\n",
    "        try:\n",
    "            mais_comentarios = driver.find_element_by_xpath(\"//*[@class='glbComentarios-lista glbComentarios-lista-recentes']/ul/li/div[1]/div/div[3]/button[not(contains(@style,'display: none'))]\")\n",
    "            time.sleep(3)\n",
    "            if(mais_comentarios.is_displayed()):\n",
    "                ActionChains(driver).move_to_element(mais_comentarios).click(mais_comentarios).perform()\n",
    "        except NoSuchElementException as e:\n",
    "            break\n",
    "    pega_comentarios()   \n",
    "\n",
    "def botao_carrega():\n",
    "    while True:\n",
    "        try:\n",
    "            carrega_mais = driver.find_element_by_xpath('//*[@id=\"boxComentarios\"]/div[4]/button')\n",
    "            time.sleep(2)\n",
    "            if(carrega_mais.is_displayed()):\n",
    "                ActionChains(driver).move_to_element(carrega_mais).click(carrega_mais).perform()\n",
    "            else:\n",
    "                break\n",
    "        except NoSuchElementException as e:\n",
    "            break\n",
    "    botao_respostas()\n",
    "     \n",
    "\n",
    "def busca_reportagens(url):\n",
    "    pagina_globo = session.get(url)\n",
    "    soup = BeautifulSoup(pagina_globo.text, 'html.parser')\n",
    "    div_reportagens = soup.find_all(\"div\",{\"class\":\"_et\"})\n",
    "    for inside_divs in div_reportagens:\n",
    "        tag_a = inside_divs.find(\"a\")\n",
    "        href_a = tag_a.get('href')\n",
    "        if(href_a):\n",
    "            valid_href = href_a\n",
    "        links_reportagem.append(tag_a.get('href'))\n",
    "        \n",
    "    for links in limpaArrays(links_reportagem):\n",
    "        try:\n",
    "            pagina = session.get(links)\n",
    "        except:\n",
    "            time.sleep(5)\n",
    "        finally:\n",
    "            soup = BeautifulSoup(pagina.text, 'html.parser')\n",
    "            existe_boxcomentarios = soup.find(\"div\",{\"id\":\"boxComentarios\"})\n",
    "            print(links)\n",
    "            if(existe_boxcomentarios):\n",
    "                driver.get(links)\n",
    "                driver.maximize_window()\n",
    "                try:\n",
    "                    box_comentarios = driver.find_element_by_id('boxComentarios')\n",
    "                    driver.execute_script('arguments[0].scrollIntoView(true);', box_comentarios)\n",
    "                finally:\n",
    "                    try:\n",
    "                        element = WebDriverWait(driver, 10).until(\n",
    "                            EC.presence_of_element_located((By.CLASS_NAME, \"glbComentarios\"))\n",
    "                        )\n",
    "                    finally:\n",
    "                        botao_carrega()\n",
    "    driver.quit()\n",
    "\n",
    "def __init__(url):\n",
    "    busca_reportagens(url)\n",
    "    \n",
    "__init__('https://g1.globo.com/')\n",
    "gera_json()        \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
